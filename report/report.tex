\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage[ansinew]{inputenc}
\usepackage{amsmath,amsthm,amssymb,latexsym,amsfonts}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[CS 651]{Course Project}
\acmYear{2018 Winter}
\copyrightyear{2018}


\acmArticle{4}
\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
\editor{Jennifer B. Sartor}
\editor{Theo D'Hondt}
\editor{Wolfgang De Meuter}


\begin{document}
\title{Logistic Regression for Sentiment Analysis on Large Scale Social Media Posts via Apache Spark}


\author{Peng Shi}
\affiliation{%
  \institution{University of Waterloo}
}
\email{peng.shi@uwaterloo.ca}

\author{Wei Yang}
\affiliation{%
  \institution{University of Waterloo}
}
\email{w85yang@uwaterloo.ca}

\author{Masijia Qiu}
\affiliation{%
  \institution{University of Waterloo}
}
\email{m23qiu@uwaterloo.ca}

% The default list of authors is too long for headers.
% \renewcommand{\shortauthors}{Peng et al.}



\begin{abstract}

We present simple distributed implementations of logistic regression (LR) with several variants, including Gradient Descent (GD) based, Stochastic Gradient Descent (SGD) based and Mini-batch Stochastic Gradient Descent (MBSGD) based. Spark implementations with rigorous experiment results show that

\end{abstract}
%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}


\keywords{Logistics regression, gradient decent, sentiment analysis}

\maketitle



\section{Introduction}
\label{sec:introduction}

Logistic Regression (LR) is a widespread and useful machine leaning algorithm in academia and industrial for its simplicity and effectiveness. However, as the data is creasing rapidly, these machine learning algorithms and tools are needed to be adapted to fit the big data. MapReduce \cite{dean2008mapreduce} is a popular algorithm in distributed computing area and Hadoop \cite{shvachko2010hadoop} and Spark \cite{zaharia2010spark} are two open-source framework for large-scale data processing. 

Today, big data community and machine learning community have developed rapidly and several machine learning frameworks for distributed computing have emerged, including MLLib, Mahout and RHadoop \cite{witten2016data}. However, for newbie to this area, these mature but complicated machine learning libraries are hard for them due to the compact structure and they have no idea to how to "get the hand dirty". 

In this paper, we provide simple but comprehensive implementations for logistic regression and its variants including different optimization method and regularization. These implementations provide the newbie a good change to learn the basic concept of logistic regression and its spark implementations. Furthermore, the newbie can start to learn to how to modify these simple code to enhance the implementation and even provide more functions. 

We have following contributions:

We offer several simple implementations for logistic regression and its variants. These implementations could be a guidance for newbie to distributed computing and machine learning. 

We ...



\section{Background and Related Work}
\label{sec:relatedWork}

Let's start with an overview of machine learning. Given $X$ to be the input space while $Y$ the output space, the set of training samples $D = \big\{ (x_1, y_1), (x_2, y_2)...(x_n, y_n) \big\} $ from the space $X$ x $Y$ space, naming the labeled examples. Usually, $x_i$ is represents a featured vector where $x_i \in \mathbf{R}^d$. In supervised classification task, $y$ comes from a finite set. when in the binary classification, $y \in \big\{ -1, +1\big\} $. A function $f$: $X \Rightarrow Y$ describing the data characteristics is introduced in the supervised machine learning tasks. The optimized case will minimize the "loss" function $L$ that quantifiably measures the discrepancy between predicted $f(x_i)$ and the actual result $y_i$. Minimizing the quantity $\sum_{(x_i, y_i) \in D} L(f(x_i), y_i)$, th e best $f$ in the learned model is selected from a hypothesis space. Then it can be employed on previously unknown data to make predictions or offer predictive analysis. \cite{christopher2016pattern} \cite{franklin2005elements} Dealing with a two-class problem as the tweets sentiment, we apply the binary logistic regression to assign observations to  two classes. 

Three components are of most significance in the machine learning solutions: the data, feature extracted from the data, and the model. Among them, the size of the dataset is dominant given the accumulated real-world experience over the last decades. \cite{halevy2009unreasonable} \cite{lin2010data} Simple models on massive data perform better than sophisticated modes on small data. \cite{banko2001scaling} \cite{brants2007large}

The traditional machine learning assumed sequential algorithms on data are fit memory, which is no more realistic in the information bang era. Multi-core \cite{chu2007map} and cluster-based solutions \cite{agarwal2014reliable} offer new opportunities. Techniques occurs for example learning decision trees and the ensembles \cite{svore2011large}, MaxEnt models \cite{mcdonald2009efficient}, structured perceptrons \cite{mcdonald2010distributed} and so on. These approaches work well when 'data is king' for their ability to process massive amount of data. Despite the gaining popularization of large-scale learning, few published studies focus on machine learning \textit{workflows} and how such tools integrate with data management platforms. Google detects adversarial advertisements on Sculley et al. \cite{sculley2011detecting} Facebook builds its data platform on Hive. \cite{thusoo2010hive} Cohen et al. applies the integration of predictive analysis into traditional RDBMSes \cite{cohen2009mad}. 

\subsection{Logistic Regression}
\label{subsection:logistic}

Logistic regression \cite{harrell2001ordinal} transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes. Assuming a two-class problem with a training set $D = \big\{ (x_1, y_1), (x_2, y_2)...(x_n, y_n) \big\} $, where $x_i$ are feature vectors and $y_i \in \big\{ -1, +1 \big\}$, a class of linear discriminative functions can be defined of the form:

\begin{equation}
F(x): \mathbf{R}^N \rightarrow \big\{ -1, +1 \big\}
\end{equation}

\begin{equation}
F(x) = \begin{cases}
    +1 if w \cdot x \geq t \\
        -1 if w \cdot x < t 
    \end{cases}
\end{equation}

where $t$ represents a decision threshold and $w$ is the weight vector. The modeling process usually optimizes the weight vector based on the training data $D$, and the decision threshold is subsequently tuned by various operational constraints. To learning $w$, lots of methods have been proposed to optimize different forms of loss functions or objective functions defined over the training data, where logistic regression is one particularly well-established technique interpreting the linear function $w \cdot x$ as the logarithmic odds of $x$ belonging to class +1 over -1, i.e., 

\begin{equation}
log[\frac{p(y=+1|x)}{p(y=-1|x)}] = w \cdot x
\end{equation}

The objective of regularized logistic regression (using Gaussian smoothing) is to identify the parameter vector $w$ that maximizes the conditional posterior of the data.

\begin{equation}
L = exp(-\lambda w^2 /2) \cdot \prod_i p( y_i | x_i )
\end{equation}

where 

\begin{equation}
p(y=+1|x)= \frac{1}{(1+exp(-w \cdot x))}
\end{equation}

and

\begin{equation}
\begin{split}
p(y=-1|x) &= 1-p(y=+1|x) \\
  &=\frac{1}{(1+exp(w \cdot x))}.
\end{split}
\end{equation}

In this project, three type of the variants are applied accordingly: Gradient Descent (GD) based, Stochastic Gradient Descent (SGD) based and Mini-batch Stochastic Gradient Descent (MBSGD) based.

\subsubsection{Gradient Descent}
\label{subsection:logisticGD}

Recall the basic Gradient Descent method, it is accomplished by adjusting the weight vector in the direction opposite to the gradient of log( $L$ )ï¼š

\begin{equation}
\begin{split}
-\bigtriangledown log(L) &= \lambda w + \sum_i \frac{1}{p(y_i |x_i )} \frac{ \vartheta}{\vartheta w} p(y_i |x_i )  \\
  &=\lambda w + \sum_i y_i p(y_i |x_i )(1-p(y_i |x_i ))
\end{split}
\end{equation}

\subsubsection{Stochastic Gradient Descent}
\label{subsection:logisticSGD}

While in GD the whole Training Set is considered before taking one Model Parameters Update Step, in SGD only one Data Point is considered for each Model Parameters Update Step, cycling over the Training Set. \cite{bottou2010large} In SGD update, the gradient is computed based on a single training instance, the update to the weight vector upon seeing the $i$th training example is given by 

\begin{equation}
w \leftarrow w + \gamma \cdot [\lambda w + y_i \cdot p( y_i | x_i )(1-p( y_i | x_i ))]
\end{equation}

Noted that each element in the weight vector is decayed at each iteration. However, when the feature vectors of training instances are very sparse (as is true for our project), we can simply delay the updates for features until they are actually seen. 

\subsubsection{Mini-Batch Stochastic Gradient Descent}
\label{subsection:logisticMBSGD}

Mini-Batch Stochastic Gradient Descent (MBSGD) \cite{ioffe2015batch} is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients. It chooses to sum the gradient over the mini-batch or take the average of the gradient which further reduces the variance of the gradient. Mini-batch gradient descent seeks to find a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent. It is the most common implementation of gradient descent used in the field of deep learning.

\subsection{Regularization}
\label{subsection:regularization}

Regularization is a concrete method for add a \textit{'penalty term'} to the optimization problem, such that more complex models includes a larger penalty. For the case of linear regression, the new optimization problem is to minimize

$$
MSE(w) + penalty(w).
$$

where penalty $w$ is increasing with the 'complexity' of $w$. Therefore, a complex solution can be chosen over simple ones only if it leads to a big decrease in the mean-squared error.

Many methods define the penalty term like \textit{ridge regression}, \textit{L2 regularization} and \textit{Tikhonov regularization}. In this project, we'll consider the most widely used one, \textit{ridge regression}.

In this method, we define

$$
penalty(w) = \lambda \cdot ||w||^2_2
$$

where $\lambda$ is a positive \textit{'hyperparameter'}, a knob that allows you to trade-off smaller MSE. For more details, we refer the reader to the authoritative paper \cite{nasrabadi2007pattern}. 

\section{Implementation}
\label{sec:implementation}

In this section, we describe our implementation details. 

With reading feature vector from text file, we push these text lines to mapper to parse and generate training and testing instances which is composed of document id (docid), label and feature vector. These instances are represented as a RDD and used for further processing. 

We describe three variants for the training processes including Gradient Descent, Stochastic Gradient Descent and Mini-Batch Stochastic Gradient Descent.

\subsection{Gradient Descent}
\label{sec:implementationGD}

With GD, the basic idea is to iterate all the training data and compute the averaged gradient descent based on global view (all training data). In general, the method to compute the averaged gradient is directly sum up all the gradient computed based on each instance and divided by the number of instances. However, because of the sparsity of the feature vectors (most of the slot of the feature vector are 0 and only small fraction are 1), the denominator will have negative effects on the magnitude of the gradient. More specifically, those feature only appear once or twice will in greatly affected because these value is divided by a large number (the total number of the instances). To tackle this problem, we compute the frequency of occurrence for each feature and use these frequencies as the regulators for the gradient computed in each iteration.

In each iteration, the global weight is broadcast to all mappers. Here the \textit{mapPatition} is used instead of \textit{map}, because in the \textit{mapPartion}, the partially sum of the gradient can be computed and the global sum of the gradient can be computed on single reducer and update the global weight. This decision can greatly lesson the load for the single reducer.

\begin{algorithm}
\caption{GD}\label{GD}
\begin{algorithmic}[h]
\Procedure{GradientDescent}{}
\State $\textit{input} \gets \text{Read from file}$
\State $\textit{inputFeatures} \gets \textit{input.map( }$
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent} $\textit{process input line and }$
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent} $\textit{generate instance with }$ 
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent} $\textit{docid, label, and features)}$
\State $\textit{globalWeight} \gets \text{Map[Int, Double]}$
\State $\textit{FeatureCounter} \gets $
\State \hspace{\algorithmicindent}  $ \text{Count Feature Frequency of Occurrence }$
\BState \emph{top}:
\State $\textit{w} \gets \textit{broadcast(globalWeight)}$
\State $\textit{inputFeatures.mapPartition(}$
\Statex \hspace{\algorithmicindent} $\textit{dw} \gets $
\Statex \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\textit{compute gradient based on }$
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\textit{this partition and sum them up}$ 
\Statex $\textit{).collect}$
\State $\textit{globalWeight} \gets \textit{sum over parital weights}$
\State \textbf{goto} \emph{top}.
\EndProcedure
\end{algorithmic}
\end{algorithm}



\subsection{Stochastic Gradient Descent}
\label{sec:implementationSGD}

For Stochastic Gradient Descent (SGD), we add "dumpy key" for each instances to ensure all the training instances are collected to the reducer via \textit{groupByKey}. After receiving all the training instance, the weight is updated per instance. 


\begin{algorithm}
\caption{SGD}\label{SGD}
\begin{algorithmic}[ ]
\Procedure{MiniBatchSGD}{}
\State $\textit{input} \gets \text{Read from file}$
\State $\textit{inputFeatures} \gets \textit{input.map( }$
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent} $\textit{process input line and }$
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent} $\textit{generate instance with }$ 
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent} $\textit{docid, label, and features)}$
\State $\textit{globalWeight} \gets \text{Map[Int, Double]}$
\BState \emph{top}:
\State $\textit{w} \gets \textit{broadcast(globalWeight)}$
\State $\textit{samples} \gets \textit{inputFeatures.sample(fraction)}$
\State $\textit{FeatureCounter} \gets $
\State \hspace{\algorithmicindent}  $ \text{Count Feature Frequency of Occurrence }$
\Statex \hspace{\algorithmicindent} $\textit{dw} \gets $
\Statex \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\textit{compute gradient based on }$
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\textit{this sample and sum them up }$ 
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent} \hspace{\algorithmicindent} $\textit{and regularized}$
\State $\textit{globalWeight} \gets \textit{sum over parital weights}$
\State \textbf{goto} \emph{top}.
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{MBSGD}\label{MBSGD}
\begin{algorithmic}[ ]
\Procedure{StochasticGradientDescent}{}
\State $\textit{input} \gets \text{Read from file}$
\State $\textit{inputFeatures} \gets \textit{input.map( }$
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent} $\textit{process input line and }$
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent} $\textit{generate instance with }$ 
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent} $\textit{docid, label, and features)}$
\State $\textit{globalWeight} \gets \text{Map[Int, Double]}$
\BState \emph{top}:
\State $\textit{for each instance}$
\State \hspace{\algorithmicindent} $\textit{dw} \gets $
\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\textit{compute gradient based on }$
\Statex \hspace{\algorithmicindent} \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\textit{this instance}$ 
\State \hspace{\algorithmicindent} $\textit{globalWeight} \gets \textit{update with dw}$
\State \textbf{goto} \emph{top}.
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Mini-Batch Stochastic Gradient Descent}
\label{sec:implementationMBGD}

The Mini-Batch Stochastic Gradient Descent (MBSGD) is the middle ground for GD and SGD. For the implementation, we use \textit{sample} to generate a small batch of training instance and apply the same implementation as the GD in the following step. More specifically, the averaged gradient is computed based on the small batch training instances and apply the update rules to the global weights. We need to notice that the denominator for each feature is counted based on this small batch size. 



\subsection{Data Processing}
\label{subsection:dataProcessing}

We generate training and testing data in following procedures: First, we filter out the tweets whose language fields are English and classify them into two categories regarding their sentiment, which is determined by "emoticon trick". More specifically, if the tweet contains a emoji or emoticons which is in our positive list, then it will be put into positive pool, and in similar way we can generate negative pool. Those tweets tweets which do not contain any emoticons or emoji, or only contain those emoticons and emoji which are not appearing in our list, will not be considered in our experiments. Secondly, we regard each as a byte array and move a four-byte sliding window, with stride equals to one, along the array, and hash the contents of the bytes, the value of which is taken as the feature id. Features are treated as binary. 

After preprocessing, the  data is split into training and test set by the ratio 8:2. We try to ensure that the numbers of positive and negative samples in both training and test set are equal. The statistics of the clean dataset can be viewed in Table \ref{statistics}.

\begin{table*}[t]
\centering
\caption{Dataset Statistics}
\label{statistics}
\begin{tabular}{c|c|c|c|c}
          &\bf TRAIN\_ALL &\bf TEST\_ALL &\bf TRAIN\_SMALL &\bf TEST\_SMALL \\ \hline \hline
File Size & 25 GB      & 6.1 GB    & 361 MB       & 73 MB       \\ \hline
\# of Positive Tweets & 33,870,264 & 8,467,134 & 500,206      & 99,918      \\ \hline
\# of Negative Tweets & 33,869,640 & 8,467,758 & 499,794      & 100,082     \\ 
\end{tabular}
\end{table*}


\subsection{Results}
\label{subsection:results}

The best performance of three variants of gradient decent is shown in Table \ref{res}.

\begin{table}[t]
\centering
\caption{Results }
\label{res}
\begin{tabular}{c|c|c}
         &\bf TEST\_ALL &\bf TEST\_SMALL \\ \hline \hline
GD &  & \\
SGD &  & \\
MBSGD &  & 
\end{tabular}
\end{table}


\subsection{Parameter Analysis}
The parameter analysis of learning rate $\eta$ is shown in Figure \ref{lr}. We compare three GD variants with different learning rates ($\eta = 0.002, 0.01, 0.05$) on the small version of dataset. Other parameters are kept fixed ($\lambda$=0, epoch=3). From the result we can see that tuning the learning rate does help achieve better performance and different GD variants has different optimal learning rates.

\begin{figure}[ht]
\centering
\includegraphics[width=0.48\textwidth]{lr.png}
\caption{Parameter Analysis on Learning Rate}
\label{lr}
\end{figure}

The parameter analysis of regularization coefficient $\lambda$ is shown in Figure \ref{reg}. We compare three GD variants with different regularization coefficients ($\lambda = 0.00001, 0.0001, 0.001, 0.01, 0.05, 0.5$) on the small version of dataset. Other parameters are kept fixed ($\eta$=0, $N$=1). From the result we can see that tuning the regularization coefficient helps little for the final result and large $\lambda$ will harm the performance.

\begin{figure}[ht]
\centering
\includegraphics[width=0.48\textwidth]{reg.png}
\caption{Parameter Analysis on Regularization Coefficient}
\label{reg}
\end{figure}

The parameter analysis of epoch $N$ is shown in Figure \ref{reg}. We compare three GD variants with different number of epochs ($N = 1, 2, 3, 4$) on the small version of dataset. Other parameters are kept fixed ($\eta$=0, $\lambda$=0.002). From the result we can see that after one epoch, on the other words, after the model goes through the whole dataset for one pass, the performance tends to converge.

\begin{figure}[ht]
\centering
\includegraphics[width=0.48\textwidth]{epoch.png}
\caption{Parameter Analysis on Epoch}
\label{epoch}
\end{figure}

\section{Conclusion and Future Work}
\label{sec:conclusionAndFutureWork}

In this project, we summarize our contributions in three points:

\begin{enumerate}  
\item We collect and clean the English stream of tweets text from February 2013 to April 2018 and create the training and test dataset for sentiment analysis with the label infromation from emojis and emoticons.
\item We extract the four byte character feature for each tweet and implement the logistics regression with GD, SGD amd mini-batch SGD for the sentiment analysis on the large scale social media data.
\item We provide the parameter analysis for the three variants of gradient decent on a subset of the whole data.
\end{enumerate}

In the future, we plan to implement more fancy optimization tricks such as momentum, nesterov and so on. We are also interested in comparing the time efficiency of different variants of gradient decent under the setup of large-scale training data.


% OBLIGATORY: use BIBTEX formatting!
\normalsize

{\vskip 12pt}
\noindent


\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-bibliography}

\end{document}
